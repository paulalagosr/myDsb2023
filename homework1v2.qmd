---
title: "Homerwork 1"
author: "PAULA LAGOS"
date: 2023-05-14
format: 
  docx: default
  html:
    toc: true
    toc_float: true
    code-fold: true
editor: visual
---

```{r}
#| label: load-libraries
#| echo: false # This option disables the printing of code (only output is displayed).
#| message: false
#| warning: false

library(tidyverse)
library(nycflights13)
library(skimr)
data(flights)
```

# Data Manipulation

## Problem 1: Use logical operators to find flights that:

```         
-   Had an arrival delay of two or more hours (\> 120 minutes)
-   Flew to Houston (IAH or HOU)
-   Were operated by United (`UA`), American (`AA`), or Delta (`DL`)
-   Departed in summer (July, August, and September)
-   Arrived more than two hours late, but didn't leave late
-   Were delayed by at least an hour, but made up over 30 minutes in flight
```

```{r}
#| label: problem-1
#First, we look at the data with VIEW, and we identify the important variables. 
view(flights)
# Had an arrival delay of two or more hours (> 120 minutes)

#I create Problem 1. We are looking for lines with the variable ARR_DELAY that show more than 120 minutes. We also sort by that variable. 
flights %>% 
  filter(arr_delay>120) %>% 
  arrange(arr_delay)



# Flew to Houston (IAH or HOU) - We are looking at the arrival airport (DEST) to be IAH or HOU
Question1.2<-flights %>% 
  filter(dest=="IAH"|dest=="HOU")


# Were operated by United (`UA`), American (`AA`), or Delta (`DL`)-  This means the carrier (carrier) is one of those
Question1.3<-flights %>% 
  filter(carrier=="UA"|carrier=="AA"|carrier=="DL")


# Departed in summer (July, August, and September). In here we look for the variable month to be 7, 8 or 9
Question1.4<-flights %>% 
  filter(month==7|month==8|month==9)
  
  
# Arrived more than two hours late, but didn't leave late. Has to have a arr_delay of 120 or more, and a dep_delay of 0 or less
Question1.5<-flights %>% 
  filter(arr_delay>120) %>%
  filter(dep_delay<=0)
  


# Were delayed by at least an hour, but made up over 30 minutes in flight. This means dep_delay is bigger than arr_delay by more than 30
Question1.6<-flights %>%
  filter(dep_delay>arr_delay+30)
  
```

## Problem 2: What months had the highest and lowest proportion of cancelled flights? Interpret any seasonal patterns. To determine if a flight was cancelled use the following code

<!-- -->

```         
flights %>% 
  filter(is.na(dep_time)) 
```

```{r}
#| label: problem-2

# What months had the highest and lowest % of cancelled flights?
#If cancelled flights the ones with no information on dep_time, I need to count by month those with na, and then, calculate the division of said number over total flights for that month


# Calculating the number of cancelled flights per month on cancelled_flights column
library(nycflights13)
library(dplyr)
Question2.1cancelled<- flights %>%
  group_by(month) %>%
  summarise(cancelled_flights = sum(is.na(dep_delay)))
  

# Calculate the total number of flights by month con total_flights column
Q2.1flights_per_month <- flights %>% 
  group_by(month) %>% 
  summarise(total_flights = n())

# Join the two data frames (left by month) and calculate the percentage of cancelled flights on a new table perc_cancelled_per_month
perc_cancelled_per_month <- left_join(Question2.1cancelled, Q2.1flights_per_month, by = "month") %>% 
  mutate(cancelled_flights_percentage= cancelled_flights/ total_flights) %>% 
  arrange(desc(cancelled_flights_percentage))
#by looking at the table, the months 8 to 11 seem to have the lower percentage of cancelled flights, but the ones on the higher end don't seem to follow a pattern


# Find the month with the highest percentage of cancelled flights
max_cancelled <- perc_cancelled_per_month %>% 
  filter(cancelled_flights_percentage == max(cancelled_flights_percentage))

# Find the month with the lowest percentage of cancelled flights
min_cancelled<- perc_cancelled_per_month %>% 
  filter(cancelled_flights_percentage == min(cancelled_flights_percentage))

# Print the results
cat("Month with the highest percentage of cancelled flights:", max_cancelled$month, "\n")
cat("Month with the lowest percentage of cancelled flights:", min_cancelled$month, "\n")


```

## Problem 3: What plane (specified by the `tailnum` variable) traveled the most times from New York City airports in 2013? Please `left_join()` the resulting table with the table `planes` (also included in the `nycflights13` package).

For the plane with the greatest number of flights and that had more than 50 seats, please create a table where it flew to during 2013.

```{r}
#Identify NYC airport codes (EWR, JFK, LGA) and count the lines any time the origin is one of those, groupy by tail number
library(nycflights13)
library(dplyr)
Q3_flights_fromNYC<-flights %>% 
  filter(year==2013,origin %in% c("EWR","JFK","LGA")) %>% 
  group_by(tailnum) %>% 
  summarise(Flights_from_NYC=n()) %>% 
  arrange(desc(Flights_from_NYC)) %>% 

#Filter only for the highest at this variable
max_flightsNYC <- Q3_flights_fromNYC %>%
  slice(2) %>% 
#LEftjoin with planes table to show information on that plane
  left_join(planes,by="tailnum")
#WE SEE THAT THE ERROR LINE WITH NA IS THE ONE WITH MORE COINCIDENCES, WE WANT TO FILTER ONLY FOR VALUES DIFFERENT FROM NA.
```

## Problem 4: The `nycflights13` package includes a table (`weather`) that describes the weather during 2013. Use that table to answer the following questions:

```         
-   What is the distribution of temperature (`temp`) in July 2013? Identify any important outliers in terms of the `wind_speed` variable.
-   What is the relationship between `dewp` and `humid`?
-   What is the relationship between `precip` and `visib`?
```

```{r}
#distribution of temperature in July 2013: identify if they 
#Identify outliers of wind_speed
#relationship betweer dewp and humid
#relationship betweer precip and visib
#From ChatGPT:

# Load required libraries
library(nycflights13)
library(dplyr)
library(ggplot2)

# Filter for weather data in July 2013
weather_jul13 <- weather %>%
  filter(month == 7, year == 2013)

# Plot a histogram of temperature
ggplot(weather_jul13, aes(x = temp)) + 
  geom_histogram(binwidth = 1, color = "white", fill = "blue") +
  labs(title = "Distribution of Temperature in July 2013",
       x = "Temperature (Fahrenheit)",
       y = "Frequency")
#[FIX THIS]WE SEE THAT THE TEMPERATURE DISTRIBUTES NORMAL WITH MEAN AROUND 78 AND DESV EST AROUND 4

# Identify outliers in terms of wind_speed
ggplot(weather_jul13, aes(y = wind_speed, x = temp)) +
  geom_boxplot(fill = "lightblue", color = "white") +
  scale_y_continuous(limits = c(0, max(weather_jul13$wind_speed) + 5)) +
  labs(title = "Wind Speed by Temperature in July 2013",
       x = "Temperature (Fahrenheit)",
       y = "Wind Speed (mph)")
#[FIX THIS]WE SEE THAT THE THERE ARE OUTLIERS 

#To evaluate a relationship betweer dewp and humid, will build a scatter plot to look for correlation. Another option could be to calculate correlation betweeen variables. 

library(nycflights13)
library(dplyr)
library(ggplot2)

# Create a scatterplot of dewp vs. humid with trend line
ggplot(weather, aes(x = dewp, y = humid)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(title = "Scatterplot of dewp vs. humid",
       x = "Dew Point Temperature (Fahrenheit)",
       y = "Humidity (%)")
#We see a positive correlation, now I will Evaluate strength of correlation

library(nycflights13)
library(dplyr)

# Fit a linear regression model for dewp vs. humid
model <- lm(humid ~ dewp, data = weather)

# Print the summary of the model
summary(model)

# Extract the R-squared value from the summary of the model
r_squared <- summary(model)$r.squared
cat("R-squared value:", r_squared)

#With an R-squared of 0.2623 we can see that the fit is not very strong. 

# Load required libraries
library(nycflights13)
library(dplyr)
library(ggplot2)

# Create a scatterplot of precip vs. visib with trend line
ggplot(weather, aes(x = precip, y = visib)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(title = "Scatterplot of precip vs. visib",
       x = "Precipitation (inches)",
       y = "Visibility (miles)")

# Calculate the Pearson correlation coefficient between precip and visib
correlation <- cor(weather$precip, weather$visib)

cat("Pearson correlation coefficient:", correlation)
#with a correlacion coefficient of -0.3199 there is not much evidence of a strong correlation, but there is a negative relationship. 
```

## Problem 5: Use the `flights` and `planes` tables to answer the following questions:

```         
-   How many planes have a missing date of manufacture?
-   What are the five most common manufacturers?
-   Has the distribution of manufacturer changed over time as reflected by the airplanes flying from NYC in 2013? (Hint: you may need to use case_when() to recode the manufacturer name and collapse rare vendors into a category called Other.)
```

```{r}
# Load required packages
library(nycflights13)
library(dplyr)

# Load flights and planes data
data(flights)
data(planes)

# Count the number of planes with missing date of manufacture
missing_manufacture_count <- planes %>%
  filter(is.na(year))%>%
  nrow()

missing_manufacture_count
#We see that there are 70 planes without information on the year of manufacturing


# Count the number of planes for each manufacturer
manufacturer_counts <- planes %>% 
  count(manufacturer, sort = TRUE)
#[******] NO ME GUSTAN ESTAS FUNCIONES COUNT Y HEAD
# Get the top 5 manufacturers
top_manufacturers <- head(manufacturer_counts$manufacturer, 5)

top_manufacturers

# Filter flights for the year 2013
flights_2013 <- flights %>%
  filter(year == 2013)

# Join flights with planes using tailnum
flights_planes_2013 <- flights_2013 %>%
  left_join(planes, by = "tailnum")

# Recode rare vendors into "Other" category using case_when()
flights_planes_2013 <- flights_planes_2013 %>%
  mutate(manufacturer = case_when(
    manufacturer %in% top_manufacturers ~ manufacturer,
    TRUE ~ "Other"
  ))

# Count the number of flights for each manufacturer
manufacturer_counts_2013 <- flights_planes_2013 %>%
  count(manufacturer, sort = TRUE)

manufacturer_counts_2013

#[*****]ELIMINAR DE ACA PARA ABAJO 
# Count the number of NA values in the year variable
sum(is.na(flights$year))
#the answer is none
#validating 
planes %>% 
  arrange(year)
planes %>% 
  arrange(desc(year))

#What are the five most common manufacturers?we try to count every line and group by manufacturer. Then, get the maximum value. 
#Using planes table
Q5Planes_per_manufacturer<-planes %>%
  group_by(manufacturer) %>% 
  summarise(Planes_per_maunf=sum(n())) %>% 
  arrange(desc(Planes_per_maunf)) %>% 
  slice(1:5)
  
#[FIX THIS] *** need to solve a question here
  

```

## Problem 6: Use the `flights` and `planes` tables to answer the following questions:

```         
-   What is the oldest plane (specified by the tailnum variable) that flew from New York City airports in 2013?
-   How many airplanes that flew from New York City are included in the planes table?
```

```{r}
#What is the oldest plane (specified by the tailnum variable) that flew from New York City airports in 2013?
 
# Filter flights for the year 2013
flights_2013 <- flights %>%
  filter(year == 2013)

# Join flights with planes using tailnum
flights_planes_2013 <- flights_2013 %>%
  left_join(planes, by = "tailnum")
#fIlter flights departing from NYC
flights_nyc_planes13<-flights_planes_2013 %>% 
  filter(origin %in% c("EWR","JFK","LGA"))

# Find the oldest plane based on the year of manufacture
oldest_plane <- flights_nyc_planes13 %>%
  filter(!is.na(year.y)) %>%
  arrange(year.y) %>%
  slice(1)

oldest_plane$tailnum

#How many airplanes that flew from New York City are included in the planes table?
# Count the number of unique airplanes in the planes table
unique_airplanes <- flights_nyc_planes13 %>%
  distinct(tailnum) %>%
  nrow()

unique_airplanes

```

## Problem 7: Use the `nycflights13` to answer the following questions:

```         
-   What is the median arrival delay on a month-by-month basis in each airport?
-   For each airline, plot the median arrival delay for each month and origin airport.
```

```{r}
library(nycflights13)
library(dplyr)

# Calculate median arrival delay on a month-by-month basis in each airport
median_arrival_delay <- flights %>%
  group_by(month, dest) %>%
  summarize(median_arr_delay = median(arr_delay, na.rm = TRUE))

median_arrival_delay

#For each airline, plot the median arrival delay for each month and origin airport.
library(ggplot2)

# Calculate median arrival delay for each airline, month, and origin airport
median_arrival_delay_airline <- flights %>%
  group_by(month, carrier, origin) %>%
  summarize(median_arr_delay = median(arr_delay, na.rm = TRUE))

# Plot the median arrival delay for each airline
ggplot(median_arrival_delay_airline, aes(x = month, y = median_arr_delay, color = origin)) +
  geom_line() +
  facet_wrap(~ carrier, nrow = 3) +
  labs(x = "Month", y = "Median Arrival Delay", color = "Origin") +
  theme_minimal()

```

## Problem 8: Let's take a closer look at what carriers service the route to San Francisco International (SFO). Join the `flights` and `airlines` tables and count which airlines flew the most to SFO. Produce a new dataframe, `fly_into_sfo` that contains three variables: the `name` of the airline, e.g., `United Air Lines Inc.` not `UA`, the count (number) of times it flew to SFO, and the `percent` of the trips that that particular airline flew to SFO.

```{r}


#With the help of Microsoft's Bing Chat answer, after iterating +5 times with chatGPT
library(nycflights13)

fly_into_sfo <- flights %>%
  #filtering landing at SFO
  filter(dest == "SFO") %>% 
  #showwing info by carrier
  group_by(carrier) %>%
  #calculating total flights per carrier
  summarise(count = n()) %>%
  #creating new column of percentage of that number over total
  mutate(percent = count / sum(count) * 100) %>%
  #Adding information of names by the key carrier
  left_join(airlines, by = "carrier") %>%
  #selecting columns to show, no code, only name and calculations
  select(name, count, percent) %>% 
  #arranging by count
  arrange(percent)


fly_into_sfo
```

And here is some bonus ggplot code to plot your dataframe

```{r}
#| label: ggplot-flights-toSFO
#| message: false
#| warning: false
library(forcats)
fly_into_sfo %>% 
  
  # sort 'name' of airline by the numbers it times to flew to SFO
  mutate(name = fct_reorder(name, count)) %>% 
  
  ggplot() +
  
  aes(x = count, 
      y = name) +
  
  # a simple bar/column plot
  geom_col() +
  
  # add labels, so each bar shows the % of total flights 
  geom_text(aes(label = percent),
             hjust = 1, 
             colour = "white", 
             size = 5)+
  
  # add labels to help our audience  
  labs(title="Which airline dominates the NYC to SFO route?", 
       subtitle = "as % of total flights in 2013",
       x= "Number of flights",
       y= NULL) +
  
  theme_minimal() + 
  
  # change the theme-- i just googled those , but you can use the ggThemeAssist add-in
  # https://cran.r-project.org/web/packages/ggThemeAssist/index.html
  
  theme(#
    # so title is left-aligned
    plot.title.position = "plot",
    
    # text in axes appears larger        
    axis.text = element_text(size=12),
    
    # title text is bigger
    plot.title = element_text(size=18)
      ) +

  # add one final layer of NULL, so if you comment out any lines
  # you never end up with a hanging `+` that awaits another ggplot layer
  NULL
 
 
```

## Problem 9: Let's take a look at cancellations of flights to SFO. We create a new dataframe `cancellations` as follows

```{r}

cancellations <- flights %>% 
  
  # just filter for destination == 'SFO'
  filter(dest == 'SFO') %>% 
  
  # a cancelled flight is one with no `dep_time` 
  filter(is.na(dep_time))

```

I want you to think how we would organise our data manipulation to create the following plot. No need to write the code, just explain in words how you would go about it.

-   First ask ggplot and select data

<!-- -->

-   define x axis as months, and Y axis as count, or number of cancelled flights

-   Then define the type of graph as bars

-   show the information for every airline name and takeoff airport pair with wrap

-   eliminate labels on

-   

![](images/sfo-cancellations.png)

## Problem 10: On your own -- Hollywood Age Gap

The website https://hollywoodagegap.com is a record of *THE AGE DIFFERENCE IN YEARS BETWEEN MOVIE LOVE INTERESTS*. This is an informational site showing the age gap between movie love interests and the data follows certain rules:

-   The two (or more) actors play actual love interests (not just friends, coworkers, or some other non-romantic type of relationship)
-   The youngest of the two actors is at least 17 years old
-   No animated characters

The age gaps dataset includes "gender" columns, which always contain the values "man" or "woman". These values appear to indicate how the characters in each film identify and some of these values do not match how the actor identifies. We apologize if any characters are misgendered in the data!

The following is a data dictionary of the variables used

| variable            | class     | description                                                                                             |
|:-----------|:-----------|:-----------------------------------------------|
| movie_name          | character | Name of the film                                                                                        |
| release_year        | integer   | Release year                                                                                            |
| director            | character | Director of the film                                                                                    |
| age_difference      | integer   | Age difference between the characters in whole years                                                    |
| couple_number       | integer   | An identifier for the couple in case multiple couples are listed for this film                          |
| actor_1\_name       | character | The name of the older actor in this couple                                                              |
| actor_2\_name       | character | The name of the younger actor in this couple                                                            |
| character_1\_gender | character | The gender of the older character, as identified by the person who submitted the data for this couple   |
| character_2\_gender | character | The gender of the younger character, as identified by the person who submitted the data for this couple |
| actor_1\_birthdate  | date      | The birthdate of the older member of the couple                                                         |
| actor_2\_birthdate  | date      | The birthdate of the younger member of the couple                                                       |
| actor_1\_age        | integer   | The age of the older actor when the film was released                                                   |
| actor_2\_age        | integer   | The age of the younger actor when the film was released                                                 |

```{r}


age_gaps <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-02-14/age_gaps.csv')


```

How would you explore this data set? Here are some ideas of tables/ graphs to help you with your analysis

Answer:

-   How is `age_difference` distributed? What's the 'typical' `age_difference` in movies?

-   The `half plus seven\` rule. Large age disparities in relationships carry certain stigmas. One popular rule of thumb is the [half-your-age-plus-seven](https://en.wikipedia.org/wiki/Age_disparity_in_sexual_relationships#The_.22half-your-age-plus-seven.22_rule) rule. This rule states you should never date anyone under half your age plus seven, establishing a minimum boundary on whom one can date. In order for a dating relationship to be acceptable under this rule, your partner's age must be:

$$\frac{\text{Your age}}{2} + 7 < \text{Partner Age} < (\text{Your age} - 7) * 2$$ How frequently does this rule apply in this dataset?

-   Which movie has the greatest number of love interests?
-   Which actors/ actresses have the greatest number of love interests in this dataset?
-   Is the mean/median age difference staying constant over the years (1935 - 2022)?
-   How frequently does Hollywood depict same-gender love interests?

# Deliverables

There is a lot of explanatory text, comments, etc. You do not need these, so delete them and produce a stand-alone document that you could share with someone. Render the edited and completed Quarto Markdown (qmd) file as a Word document (use the "Render" button at the top of the script editor window) and upload it to Canvas. You must be commiting and pushing tour changes to your own Github repo as you go along.

# Details

-   Who did you collaborate with: Just me, with the help of Chat GPT and Microsoft Bing Chat
-   Approximately how much time did you spend on this problem set: ANSWER HERE
-   What, if anything, gave you the most trouble: multiple lines to answer one question, such as calculations, filter, etc. Remembering to checck the data for NA, or errors, validating my answers

**Please seek out help when you need it,** and remember the [15-minute rule](https://mam2022.netlify.app/syllabus/#the-15-minute-rule){target="_blank"}. You know enough R (and have enough examples of code from class and your readings) to be able to do this. If you get stuck, ask for help from others, post a question on Slack-- and remember that I am here to help too!

> As a true test to yourself, do you understand the code you submitted and are you able to explain it to someone else?

# Rubric

13/13: Problem set is 100% completed. Every question was attempted and answered, and most answers are correct. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you've written additional text to describe how you interpret the output. Multiple Github commits. Work is exceptional. I will not assign these often.

8/13: Problem set is 60--80% complete and most answers are correct. This is the expected level of performance. Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output). A few Github commits.

5/13: Problem set is less than 60% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not assign these often. Displays minimal effort. Doesn't complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn't use plots appropriate for the variables being analyzed. No Github commits.
